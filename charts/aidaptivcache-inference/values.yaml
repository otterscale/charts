image:
  repository: docker.io/library/aidaptiv
  tag: vNXUN_3_03BAA-1
  pullPolicy: IfNotPresent

imagePullSecrets: []

# Deployment configuration
deployment:
  name: vllm-api
  replicas: 1

# vLLM server configuration
vllm:
  # Environment variables
  env:
    vllmUseV1: "1"
    vllmWorkerMultiprocMethod: "spawn"
    tiktokenEncodingsBase: ""
  
  # Command line arguments
  args:
    model: /mnt/data/model/Meta-Llama-3.1-8B-Instruct/
    nvmePath: /mnt/nvme0
    port: 8000
    gpuMemoryUtilization: 0.9
    maxModelLen: 32768
    tensorParallelSize: 4
    dramKvOffloadGb: 0
    ssdKvOffloadGb: 500
    noResumeKvCache: true
    disableGpuReuse: false
    enableChunkedPrefill: true
    # Optional flags (commented out by default)
    # disableLongToken: true
    # resumeKvCache: true
    # cleanObsoleteKvCache: true
    # enablePrefixCaching: true
    # enforceEager: true
  
  # LoRA configuration (all three parameters must be set together)
  lora:
    enable: false  # Set to true to enable LoRA
    modules: ""  # Example: "lora=/app/llama3.1-8B-lora/" (required if enable is true)
    maxRank: 32  # Maximum LoRA rank (required if enable is true)

# Service configuration
service:
  type: NodePort
  port: 8000
  targetPort: 8000
  # Optional: uncomment to set a specific nodePort
  # nodePort: 30299

# Security context
securityContext:
  privileged: true

# Volume configuration
volumes:
  dshm:
    enabled: true
    sizeLimit: 30Gi
  nfs:
    enabled: true  
    server: ""  # NFS server and path (e.g., "10.102.197.0:/volumes/_nogroup/test/a752f0eb-bfd6-4395-a629-4b91144b41e6")
    mountPath: /mnt/data  # Local mount path for model
    nfsVersion: "4.1"  # NFS version

# Resource requests and limits
resources:
  requests:
    otterscale.com/vgpu: 1
    otterscale.com/vgpumem: 2048
    #otterscale.com/vgpumem-percentage: 
    phison.com/ai100: 1
  limits:
    otterscale.com/vgpu: 1
    otterscale.com/vgpumem: 2048
    #otterscale.com/vgpumem-percentage: 
    phison.com/ai100: 1

# Node selector for GPU nodes
nodeSelector: {}

# Tolerations for GPU taints
tolerations: []

# Affinity rules
affinity: {}
